---
name: torchtalk-analyzer
description: Get cross-language binding information for PyTorch codebases (Python → C++ → CUDA). Use when you need to understand dispatch paths, find backend implementations (CPU/CUDA), trace CUDA kernels, or understand how Python APIs connect to native code. Provides structural architectural data, not code search.
allowed-tools: mcp__torchtalk__get_status, mcp__torchtalk__trace, mcp__torchtalk__search, mcp__torchtalk__impact, mcp__torchtalk__calls, mcp__torchtalk__called_by, mcp__torchtalk__cuda_kernels, Read, Grep, Glob
---

# TorchTalk Cross-Language Binding Analyzer

## When to Use This Skill

Use TorchTalk when asked about:
- How a PyTorch function works internally
- What would break if something is modified (impact analysis)
- Where a function is implemented (Python/C++/CUDA)
- How Python APIs connect to native code

## Quick Start: Check Status First

If unsure what's available, start with:
```
get_status()
```
This shows what's loaded and any setup needed.

## Primary Tools

### `trace(function_name, focus?)`
**Use for:** "How does X work?" / "Where is X implemented?"

Shows complete Python → YAML → C++ → file mapping.
- `focus="full"` (default): Everything
- `focus="yaml"`: Just native_functions.yaml definition
- `focus="dispatch"`: Just backend registrations

```
trace("matmul")
→ Definition from native_functions.yaml
→ Dispatch: CompositeImplicitAutograd → matmul
→ Implementation: aten/src/ATen/native/LinearAlgebra.cpp:1996
```

### `impact(function_name, depth?)` ⭐ Security/Refactoring
**Use for:** "What would break if I change X?" / "What's the blast radius?"

Traces **transitive** callers up to `depth` levels (default 3, max 5).
Also finds Python entry points that eventually call the target.

```
impact("gemm", depth=3)
→ Depth 1: addmm, bmm, mm (direct callers)
→ Depth 2: linear, matmul (callers of callers)
→ Depth 3: transformer layers...
→ Python Entry Points: torch.nn.Linear, torch.matmul
```

### `called_by(function_name)`
**Use for:** "What directly calls X?" (single level)

Shows immediate callers with file:line locations.

```
called_by("gemm")
→ at::native::cpublas::brgemm at CPUBlas.cpp:1347
→ at::native::addmm at LinearAlgebra.cpp:892
```

### `calls(function_name)`
**Use for:** "What does X call internally?" / "What are X's dependencies?"

Shows functions this calls with file:line locations.

```
calls("_matmul_impl")
→ at::Tensor::mm, at::Tensor::mv, at::Tensor::dot
```

### `search(query, backend?)`
**Use for:** "Find functions related to X"

Search bindings by name with optional backend filter.

```
search("conv", backend="CUDA")
→ Only CUDA implementations of conv-related functions
```

### `cuda_kernels(function_name?)`
**Use for:** "What GPU kernels does X use?"

Shows `__global__` CUDA kernels with `<<<>>>` launches.

## Workflow Examples

### Example 1: "How does torch.matmul work?"

1. Get the binding chain:
   ```
   trace("matmul")
   ```
2. Get internal calls:
   ```
   calls("matmul")
   ```
3. Read the implementation file shown in results
4. Answer with specific code references

### Example 2: "What would break if I modify the GEMM implementation?"

1. Get full impact analysis:
   ```
   impact("gemm", depth=4)
   ```
2. Review Python entry points to understand user-facing impact
3. Explain the impact chain from low-level to high-level

### Example 3: "Where is conv2d implemented for CUDA?"

1. Search with backend filter:
   ```
   search("conv2d", backend="CUDA")
   ```
2. Or get full trace:
   ```
   trace("conv2d", focus="dispatch")
   ```
3. Read the CUDA file

## Tool Status

Some tools require `compile_commands.json` (generated by building PyTorch):

| Tool | Requires Build? | Purpose |
|------|----------------|---------|
| `trace` | No | Python→C++ mapping |
| `search` | No | Find bindings |
| `cuda_kernels` | No | GPU kernels |
| `impact` | **Yes** | Transitive callers |
| `calls` | **Yes** | Outbound dependencies |
| `called_by` | **Yes** | Inbound dependencies |

If call graph tools return "not available", the user needs to build PyTorch once. Use `get_status()` for guidance.
