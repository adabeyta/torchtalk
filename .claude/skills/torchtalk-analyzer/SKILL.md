---
name: torchtalk-analyzer
description: Get cross-language binding information for PyTorch codebases (Python → C++ → CUDA). Use when you need to understand dispatch paths, find backend implementations (CPU/CUDA), trace CUDA kernels, or understand how Python APIs connect to native code. Provides structural architectural data, not code search.
allowed-tools: mcp__torchtalk__get_status, mcp__torchtalk__get_binding_chain, mcp__torchtalk__get_native_function, mcp__torchtalk__get_dispatch_implementations, mcp__torchtalk__get_cpp_callers, mcp__torchtalk__get_cpp_callees, mcp__torchtalk__get_cuda_kernels, mcp__torchtalk__search_bindings, Read, Grep, Glob
---

# TorchTalk Cross-Language Binding Analyzer

## When to Use This Skill

Use TorchTalk when asked about:
- How a PyTorch function works internally
- What would break if something is modified (impact analysis)
- Where a function is implemented (Python/C++/CUDA)
- How Python APIs connect to native code

## Quick Start: Check Status First

If unsure what's available, start with:
```
get_status()
```
This shows what's loaded and any setup needed.

## Primary Tools

### `get_binding_chain(function_name)`
**Use for:** "How does X work?" / "Where is X implemented?"

Shows complete Python → C++ → file mapping with dispatch configuration.

```
get_binding_chain("matmul")
→ Native function definition from native_functions.yaml
→ Dispatch: CompositeImplicitAutograd → matmul
→ Implementation: aten/src/ATen/native/LinearAlgebra.cpp:1996
```

### `get_cpp_callers(function_name)` ⭐ Impact Analysis
**Use for:** "What would break if I change X?" / "What depends on X?"

Shows all C++ functions that call the target function.

```
get_cpp_callers("gemm")
→ gemm is called by:
  - at::native::cpublas::brgemm at CPUBlas.cpp:1347
  - at::native::addmm at LinearAlgebra.cpp:892
```

### `get_cpp_callees(function_name)` ⭐ Dependency Analysis
**Use for:** "What does X call internally?" / "What are X's dependencies?"

Shows what a function calls.

```
get_cpp_callees("_matmul_impl")
→ _matmul_impl calls:
  - at::Tensor::mm
  - at::Tensor::mv
  - at::Tensor::dot
  - at::Tensor::squeeze
```

### `get_native_function(function_name)`
**Use for:** Official operator definition from native_functions.yaml

Shows authoritative signature, dispatch config, and backward formula.

### `get_dispatch_implementations(function_name)`
**Use for:** "Which backend (CPU/CUDA) handles X?"

Table of all backend implementations with file locations.

### `get_cuda_kernels(function_name)`
**Use for:** "What GPU kernels does X use?"

Shows `__global__` CUDA kernels and what launches them.

### `search_bindings(query)`
**Use for:** "Find functions related to X"

Search all bindings by name.

## Workflow Examples

### Example 1: "How does torch.matmul work?"

1. Get the binding chain:
   ```
   get_binding_chain("matmul")
   ```
2. Get internal calls:
   ```
   get_cpp_callees("matmul")
   ```
3. Read the implementation file shown in results
4. Answer with specific code references

### Example 2: "What would break if I modify the GEMM implementation?"

1. Find callers:
   ```
   get_cpp_callers("gemm")
   ```
2. For each caller, check what it does:
   ```
   get_binding_chain("addmm")  # if addmm calls gemm
   ```
3. Explain the impact chain

### Example 3: "Where is conv2d implemented for CUDA?"

1. Get dispatch implementations:
   ```
   get_dispatch_implementations("conv2d")
   ```
2. Look for CUDA dispatch key in results
3. Read the CUDA file

## Tool Status

Some tools require `compile_commands.json` (generated by building PyTorch):

| Tool | Requires Build? | Purpose |
|------|----------------|---------|
| `get_binding_chain` | No | Python→C++ mapping |
| `get_native_function` | No | Operator definitions |
| `get_dispatch_implementations` | No | Backend routing |
| `get_cuda_kernels` | No | GPU kernels |
| `get_cpp_callers` | **Yes** | Impact analysis |
| `get_cpp_callees` | **Yes** | Dependency tracing |

If `get_cpp_callers`/`get_cpp_callees` return "not available", the user needs to build PyTorch once. Use `get_status()` for guidance.
